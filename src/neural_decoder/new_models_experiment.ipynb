{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from data_utils import get_lazy_dataloaders\n",
    "from model_utils import evaluate_model, train_model\n",
    "from eeg_clip_basic import EEGToCLIPNet\n",
    "from scipy.spatial.distance import correlation\n",
    "\n",
    "from diffusers import StableUnCLIPImg2ImgPipeline\n",
    "import os\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'variation': 'fp16'} are not expected by StableUnCLIPImg2ImgPipeline and will be ignored.\n",
      "Loading pipeline components...: 100%|██████████| 9/9 [00:05<00:00,  1.77it/s]\n"
     ]
    }
   ],
   "source": [
    "sub = 1\n",
    "recon_dir = f\"results/thingseeg2_preproc/sub-{sub:02d}/unclip/\" # Directory to save the reconstructed images\n",
    "os.makedirs(recon_dir, exist_ok=True)\n",
    "\n",
    "# Start the StableUnCLIP Image variations pipeline\n",
    "pipe = StableUnCLIPImg2ImgPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-2-1-unclip\", torch_dtype=torch.float16, variation=\"fp16\"\n",
    ")\n",
    "\n",
    "device = \"cuda\"\n",
    "pipe = pipe.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 17, 80]) torch.Size([32, 1024])\n",
      "torch.Size([32, 17, 80]) torch.Size([32, 36864])\n"
     ]
    }
   ],
   "source": [
    "clip_train_loader, clip_val_loader, clip_test_loader = get_lazy_dataloaders(sub_id=1, batch_size=32, shuffle=True, embedding_type='clip', flatten_eeg=False)\n",
    "\n",
    "for data in clip_val_loader:\n",
    "    print(data[0].shape, data[1].shape)\n",
    "    break\n",
    "\n",
    "vae_train_loader, vae_val_loader, vae_test_loader = get_lazy_dataloaders(sub_id=1, batch_size=32, shuffle=True, embedding_type='vae', flatten_eeg=False)\n",
    "\n",
    "for data in vae_val_loader:\n",
    "    print(data[0].shape, data[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEG_CNN1D(nn.Module):\n",
    "    def __init__(self, num_channels=17, num_time_points=80, output_dim=1024): # Assuming CLIP embedding is 768\n",
    "        super(EEG_CNN1D, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(num_channels, 64, kernel_size=5, padding=2), # Output: (batch, 64, 80)\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2), # Output: (batch, 64, 40)\n",
    "\n",
    "            nn.Conv1d(64, 128, kernel_size=5, padding=2), # Output: (batch, 128, 40)\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2), # Output: (batch, 128, 20)\n",
    "\n",
    "            nn.Conv1d(128, 256, kernel_size=5, padding=2), # Output: (batch, 256, 20)\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2), # Output: (batch, 256, 10)\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(256 * 10, output_dim) # Adjust 256*10 based on final Conv1d output shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x shape: (batch_size, channels, time_points) -> (32, 17, 80)\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   0%|          | 0/414 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 414/414 [00:01<00:00, 403.87it/s]\n",
      "Epoch 2/100: 100%|██████████| 414/414 [00:00<00:00, 536.15it/s]\n",
      "Epoch 3/100: 100%|██████████| 414/414 [00:00<00:00, 523.86it/s]\n",
      "Epoch 4/100: 100%|██████████| 414/414 [00:00<00:00, 546.84it/s]\n",
      "Epoch 5/100: 100%|██████████| 414/414 [00:00<00:00, 515.30it/s]\n",
      "Epoch 6/100: 100%|██████████| 414/414 [00:00<00:00, 500.12it/s]\n",
      "Epoch 7/100: 100%|██████████| 414/414 [00:00<00:00, 547.57it/s]\n",
      "Epoch 8/100: 100%|██████████| 414/414 [00:00<00:00, 546.31it/s]\n",
      "Epoch 9/100: 100%|██████████| 414/414 [00:00<00:00, 497.15it/s]\n",
      "Epoch 10/100: 100%|██████████| 414/414 [00:00<00:00, 515.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Train Loss: 0.291119, Val Loss: 0.294944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100: 100%|██████████| 414/414 [00:00<00:00, 544.32it/s]\n",
      "Epoch 12/100: 100%|██████████| 414/414 [00:00<00:00, 508.93it/s]\n",
      "Epoch 13/100: 100%|██████████| 414/414 [00:00<00:00, 491.46it/s]\n",
      "Epoch 14/100: 100%|██████████| 414/414 [00:00<00:00, 565.10it/s]\n",
      "Epoch 15/100: 100%|██████████| 414/414 [00:00<00:00, 505.11it/s]\n",
      "Epoch 16/100: 100%|██████████| 414/414 [00:00<00:00, 560.32it/s]\n",
      "Epoch 17/100: 100%|██████████| 414/414 [00:00<00:00, 536.08it/s]\n",
      "Epoch 18/100: 100%|██████████| 414/414 [00:00<00:00, 510.21it/s]\n",
      "Epoch 19/100: 100%|██████████| 414/414 [00:00<00:00, 522.06it/s]\n",
      "Epoch 20/100: 100%|██████████| 414/414 [00:00<00:00, 524.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100, Train Loss: 0.283914, Val Loss: 0.295039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/100: 100%|██████████| 414/414 [00:00<00:00, 500.17it/s]\n",
      "Epoch 22/100: 100%|██████████| 414/414 [00:00<00:00, 530.25it/s]\n",
      "Epoch 23/100: 100%|██████████| 414/414 [00:00<00:00, 512.19it/s]\n",
      "Epoch 24/100: 100%|██████████| 414/414 [00:00<00:00, 471.62it/s]\n",
      "Epoch 25/100: 100%|██████████| 414/414 [00:00<00:00, 495.96it/s]\n",
      "Epoch 26/100: 100%|██████████| 414/414 [00:00<00:00, 505.41it/s]\n",
      "Epoch 27/100: 100%|██████████| 414/414 [00:00<00:00, 506.52it/s]\n",
      "Epoch 28/100: 100%|██████████| 414/414 [00:00<00:00, 514.82it/s]\n",
      "Epoch 29/100: 100%|██████████| 414/414 [00:00<00:00, 515.14it/s]\n",
      "Epoch 30/100: 100%|██████████| 414/414 [00:00<00:00, 548.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100, Train Loss: 0.255710, Val Loss: 0.313020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/100: 100%|██████████| 414/414 [00:00<00:00, 516.07it/s]\n",
      "Epoch 32/100: 100%|██████████| 414/414 [00:00<00:00, 493.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([1.1050289553288677,\n",
       "  0.3844064027790862,\n",
       "  0.3131975491985607,\n",
       "  0.29855412129618697,\n",
       "  0.29554848423326646,\n",
       "  0.29407232503096264,\n",
       "  0.2931280374383005,\n",
       "  0.29236653447151184,\n",
       "  0.2918951083784518,\n",
       "  0.2911187402436123,\n",
       "  0.2904218382017624,\n",
       "  0.28986171399049715,\n",
       "  0.28936839118095986,\n",
       "  0.2888127211211384,\n",
       "  0.2885899125521886,\n",
       "  0.2883835282302709,\n",
       "  0.28803187385561385,\n",
       "  0.28760395044289927,\n",
       "  0.2850473228716044,\n",
       "  0.28391379132362954,\n",
       "  0.28302320240488377,\n",
       "  0.28188225929287897,\n",
       "  0.2802225512582899,\n",
       "  0.27869842665782874,\n",
       "  0.2730510470947782,\n",
       "  0.26965870791011387,\n",
       "  0.26629475820899584,\n",
       "  0.26271561068901117,\n",
       "  0.25913644402067443,\n",
       "  0.2557101748199854,\n",
       "  0.24758549657276863,\n",
       "  0.24382732113922276],\n",
       " [0.47420925159866995,\n",
       "  0.3336997708448997,\n",
       "  0.30385965968553835,\n",
       "  0.2988210248832519,\n",
       "  0.2970868091170604,\n",
       "  0.29597899661614346,\n",
       "  0.29546580005150574,\n",
       "  0.2950788398201649,\n",
       "  0.29495129447716933,\n",
       "  0.29494399348130595,\n",
       "  0.2946973262498012,\n",
       "  0.2936868157524329,\n",
       "  0.29378539667679715,\n",
       "  0.2948432401395761,\n",
       "  0.2948538764164998,\n",
       "  0.29454301555569357,\n",
       "  0.29558180988981175,\n",
       "  0.2947044839652685,\n",
       "  0.29578042288239187,\n",
       "  0.2950392491542376,\n",
       "  0.29678426023859245,\n",
       "  0.2970809182868554,\n",
       "  0.2982624706167441,\n",
       "  0.2992757590344319,\n",
       "  0.30099157272623134,\n",
       "  0.3023068532347679,\n",
       "  0.3042599384028178,\n",
       "  0.31040115597156376,\n",
       "  0.3135811617741218,\n",
       "  0.31302018664204156,\n",
       "  0.3177652943592805,\n",
       "  0.3208279956418734])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1d_model = EEG_CNN1D()\n",
    "train_model(conv1d_model, clip_train_loader, clip_val_loader, device='cuda', model_name='conv1d_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding for transformer\"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "\n",
    "class EEGTransformerModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-based model for EEG to CLIP embedding conversion\n",
    "    \n",
    "    Input: (batch_size, num_channels, time_freq)\n",
    "    Output: (batch_size, 1024) - CLIP embedding dimension\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_channels, \n",
    "                 time_freq, \n",
    "                 d_model=512, \n",
    "                 nhead=8, \n",
    "                 num_layers=6, \n",
    "                 dim_feedforward=2048, \n",
    "                 dropout=0.1,\n",
    "                 clip_embedding_dim=1024,\n",
    "                 use_positional_encoding=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_channels = num_channels\n",
    "        self.time_freq = time_freq\n",
    "        self.d_model = d_model\n",
    "        self.clip_embedding_dim = clip_embedding_dim\n",
    "        \n",
    "        # Input projection: project each channel to d_model dimensions\n",
    "        self.input_projection = nn.Linear(num_channels, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.use_positional_encoding = use_positional_encoding\n",
    "        if use_positional_encoding:\n",
    "            self.pos_encoder = PositionalEncoding(d_model, max_len=time_freq)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Global pooling and output projection\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.output_projection = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, clip_embedding_dim)\n",
    "        )\n",
    "        \n",
    "        # Optional: Layer normalization for output\n",
    "        self.output_norm = nn.LayerNorm(clip_embedding_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, num_channels, time_freq)\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, clip_embedding_dim)\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Transpose to (batch_size, time_freq, num_channels)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # Project channels to d_model dimensions: (batch_size, time_freq, d_model)\n",
    "        x = self.input_projection(x)\n",
    "        \n",
    "        # Add positional encoding if enabled\n",
    "        if self.use_positional_encoding:\n",
    "            x = x.transpose(0, 1)  # (time_freq, batch_size, d_model)\n",
    "            x = self.pos_encoder(x)\n",
    "            x = x.transpose(0, 1)  # (batch_size, time_freq, d_model)\n",
    "        \n",
    "        # Apply transformer encoder\n",
    "        x = self.transformer_encoder(x)  # (batch_size, time_freq, d_model)\n",
    "        \n",
    "        # Global pooling across time dimension\n",
    "        x = x.transpose(1, 2)  # (batch_size, d_model, time_freq)\n",
    "        x = self.global_pool(x)  # (batch_size, d_model, 1)\n",
    "        x = x.squeeze(-1)  # (batch_size, d_model)\n",
    "        \n",
    "        # Project to CLIP embedding dimension\n",
    "        x = self.output_projection(x)  # (batch_size, clip_embedding_dim)\n",
    "        \n",
    "        # Apply layer normalization\n",
    "        x = self.output_norm(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class EEGTransformerModelV2(nn.Module):\n",
    "    \"\"\"\n",
    "    Alternative transformer model with different architecture\n",
    "    \n",
    "    This version uses a more sophisticated approach with:\n",
    "    - Multi-scale feature extraction\n",
    "    - Residual connections\n",
    "    - Channel attention mechanism\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_channels, \n",
    "                 time_freq, \n",
    "                 d_model=512, \n",
    "                 nhead=8, \n",
    "                 num_layers=6, \n",
    "                 dim_feedforward=2048, \n",
    "                 dropout=0.1,\n",
    "                 clip_embedding_dim=1024):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_channels = num_channels\n",
    "        self.time_freq = time_freq\n",
    "        self.d_model = d_model\n",
    "        self.clip_embedding_dim = clip_embedding_dim\n",
    "        \n",
    "        # Multi-scale feature extraction\n",
    "        self.channel_conv = nn.Sequential(\n",
    "            nn.Conv1d(num_channels, d_model // 2, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(d_model // 2, d_model, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Channel attention mechanism\n",
    "        self.channel_attention = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Conv1d(d_model, d_model // 4, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(d_model // 4, d_model, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len=time_freq)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Output projection with residual connection\n",
    "        self.output_projection = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, clip_embedding_dim)\n",
    "        )\n",
    "        \n",
    "        # Global pooling\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        # Output normalization\n",
    "        self.output_norm = nn.LayerNorm(clip_embedding_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass with multi-scale feature extraction and attention\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, num_channels, time_freq)\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, clip_embedding_dim)\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Multi-scale feature extraction\n",
    "        conv_features = self.channel_conv(x)  # (batch_size, d_model, time_freq)\n",
    "        \n",
    "        # Channel attention\n",
    "        attention_weights = self.channel_attention(conv_features)\n",
    "        attended_features = conv_features * attention_weights\n",
    "        \n",
    "        # Prepare for transformer\n",
    "        x = attended_features.transpose(1, 2)  # (batch_size, time_freq, d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = x.transpose(0, 1)  # (time_freq, batch_size, d_model)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = x.transpose(0, 1)  # (batch_size, time_freq, d_model)\n",
    "        \n",
    "        # Apply transformer encoder\n",
    "        x = self.transformer_encoder(x)  # (batch_size, time_freq, d_model)\n",
    "        \n",
    "        # Global pooling\n",
    "        x = x.transpose(1, 2)  # (batch_size, d_model, time_freq)\n",
    "        x = self.global_pool(x)  # (batch_size, d_model, 1)\n",
    "        x = x.squeeze(-1)  # (batch_size, d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        x = self.output_projection(x)  # (batch_size, clip_embedding_dim)\n",
    "        \n",
    "        # Normalize output\n",
    "        x = self.output_norm(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 414/414 [00:05<00:00, 74.25it/s]\n",
      "Epoch 2/100: 100%|██████████| 414/414 [00:05<00:00, 75.34it/s]\n",
      "Epoch 3/100: 100%|██████████| 414/414 [00:05<00:00, 75.92it/s]\n",
      "Epoch 4/100: 100%|██████████| 414/414 [00:05<00:00, 75.93it/s]\n",
      "Epoch 5/100: 100%|██████████| 414/414 [00:05<00:00, 74.27it/s]\n",
      "Epoch 6/100: 100%|██████████| 414/414 [00:05<00:00, 75.01it/s]\n",
      "Epoch 7/100: 100%|██████████| 414/414 [00:05<00:00, 75.08it/s]\n",
      "Epoch 8/100: 100%|██████████| 414/414 [00:05<00:00, 74.87it/s]\n",
      "Epoch 9/100: 100%|██████████| 414/414 [00:05<00:00, 75.26it/s]\n",
      "Epoch 10/100: 100%|██████████| 414/414 [00:05<00:00, 75.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Train Loss: 0.300957, Val Loss: 0.301536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100: 100%|██████████| 414/414 [00:05<00:00, 74.61it/s]\n",
      "Epoch 12/100: 100%|██████████| 414/414 [00:05<00:00, 74.91it/s]\n",
      "Epoch 13/100: 100%|██████████| 414/414 [00:05<00:00, 74.97it/s]\n",
      "Epoch 14/100: 100%|██████████| 414/414 [00:05<00:00, 74.79it/s]\n",
      "Epoch 15/100: 100%|██████████| 414/414 [00:05<00:00, 75.39it/s]\n",
      "Epoch 16/100: 100%|██████████| 414/414 [00:05<00:00, 75.47it/s]\n",
      "Epoch 17/100: 100%|██████████| 414/414 [00:05<00:00, 74.65it/s]\n",
      "Epoch 18/100: 100%|██████████| 414/414 [00:05<00:00, 74.95it/s]\n",
      "Epoch 19/100: 100%|██████████| 414/414 [00:05<00:00, 75.56it/s]\n",
      "Epoch 20/100: 100%|██████████| 414/414 [00:05<00:00, 75.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100, Train Loss: 0.300616, Val Loss: 0.301212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/100: 100%|██████████| 414/414 [00:05<00:00, 74.90it/s]\n",
      "Epoch 22/100: 100%|██████████| 414/414 [00:05<00:00, 75.17it/s]\n",
      "Epoch 23/100: 100%|██████████| 414/414 [00:05<00:00, 75.24it/s]\n",
      "Epoch 24/100: 100%|██████████| 414/414 [00:05<00:00, 74.19it/s]\n",
      "Epoch 25/100: 100%|██████████| 414/414 [00:05<00:00, 74.74it/s]\n",
      "Epoch 26/100: 100%|██████████| 414/414 [00:05<00:00, 74.87it/s]\n",
      "Epoch 27/100: 100%|██████████| 414/414 [00:05<00:00, 74.76it/s]\n",
      "Epoch 28/100: 100%|██████████| 414/414 [00:05<00:00, 74.01it/s]\n",
      "Epoch 29/100: 100%|██████████| 414/414 [00:05<00:00, 74.07it/s]\n",
      "Epoch 30/100: 100%|██████████| 414/414 [00:05<00:00, 75.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100, Train Loss: 0.300429, Val Loss: 0.301055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/100: 100%|██████████| 414/414 [00:05<00:00, 73.83it/s]\n",
      "Epoch 32/100: 100%|██████████| 414/414 [00:05<00:00, 73.36it/s]\n",
      "Epoch 33/100: 100%|██████████| 414/414 [00:05<00:00, 74.71it/s]\n",
      "Epoch 34/100: 100%|██████████| 414/414 [00:05<00:00, 75.19it/s]\n",
      "Epoch 35/100: 100%|██████████| 414/414 [00:05<00:00, 75.69it/s]\n",
      "Epoch 36/100: 100%|██████████| 414/414 [00:05<00:00, 75.90it/s]\n",
      "Epoch 37/100: 100%|██████████| 414/414 [00:05<00:00, 74.63it/s]\n",
      "Epoch 38/100: 100%|██████████| 414/414 [00:05<00:00, 75.13it/s]\n",
      "Epoch 39/100: 100%|██████████| 414/414 [00:05<00:00, 74.93it/s]\n",
      "Epoch 40/100: 100%|██████████| 414/414 [00:05<00:00, 74.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100, Train Loss: 0.300340, Val Loss: 0.301035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/100: 100%|██████████| 414/414 [00:05<00:00, 74.75it/s]\n",
      "Epoch 42/100: 100%|██████████| 414/414 [00:05<00:00, 74.62it/s]\n",
      "Epoch 43/100: 100%|██████████| 414/414 [00:05<00:00, 74.52it/s]\n",
      "Epoch 44/100: 100%|██████████| 414/414 [00:05<00:00, 74.43it/s]\n",
      "Epoch 45/100: 100%|██████████| 414/414 [00:05<00:00, 73.24it/s]\n",
      "Epoch 46/100: 100%|██████████| 414/414 [00:05<00:00, 74.49it/s]\n",
      "Epoch 47/100: 100%|██████████| 414/414 [00:05<00:00, 74.59it/s]\n",
      "Epoch 48/100: 100%|██████████| 414/414 [00:05<00:00, 74.19it/s]\n",
      "Epoch 49/100: 100%|██████████| 414/414 [00:05<00:00, 74.45it/s]\n",
      "Epoch 50/100: 100%|██████████| 414/414 [00:05<00:00, 75.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100, Train Loss: 0.300222, Val Loss: 0.300958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/100:  14%|█▍        | 58/414 [00:00<00:04, 75.61it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m transformer_model = EEGTransformerModel(num_channels=\u001b[32m17\u001b[39m, time_freq=\u001b[32m80\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformer_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_val_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtransformer_model\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/neural-decoder/src/neural_decoder/model_utils.py:24\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, device, num_epochs, lr, weight_decay, model_name)\u001b[39m\n\u001b[32m     22\u001b[39m model.train()\n\u001b[32m     23\u001b[39m train_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_eeg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_clip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mEpoch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_eeg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_clip\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_eeg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_clip\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/neural-decoder/.venv/lib/python3.13/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/neural-decoder/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/neural-decoder/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/neural-decoder/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/neural-decoder/.venv/lib/python3.13/site-packages/torch/utils/data/dataset.py:416\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/neural-decoder/src/neural_decoder/data_utils.py:37\u001b[39m, in \u001b[36mEEGEmbeddingLazyDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     33\u001b[39m     eeg = \u001b[38;5;28mself\u001b[39m.eeg[idx]  \u001b[38;5;66;03m# Keep original shape (e.g., [4, 63, 250])\u001b[39;00m\n\u001b[32m     35\u001b[39m     eeg = (eeg - \u001b[38;5;28mself\u001b[39m.norm_mean) / \u001b[38;5;28mself\u001b[39m.norm_std\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m embedding = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch.from_numpy(eeg.copy()).float(), torch.from_numpy(embedding.copy()).float()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/neural-decoder/.venv/lib/python3.13/site-packages/numpy/_core/memmap.py:359\u001b[39m, in \u001b[36mmemmap.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    356\u001b[39m     \u001b[38;5;66;03m# Return ndarray otherwise\u001b[39;00m\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.view(np.ndarray)\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[32m    360\u001b[39m     res = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getitem__\u001b[39m(index)\n\u001b[32m    361\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(res) \u001b[38;5;129;01mis\u001b[39;00m memmap \u001b[38;5;129;01mand\u001b[39;00m res._mmap \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "transformer_model = EEGTransformerModel(num_channels=17, time_freq=80)\n",
    "train_model(transformer_model, clip_train_loader, clip_val_loader, device='cuda', model_name='transformer_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
