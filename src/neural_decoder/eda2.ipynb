{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26551e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from natsort import natsorted\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "48759643",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_id = \"01\"\n",
    "image_dir = \"data/images\"\n",
    "eeg_dir = f\"data/things-eeg2/sub-{sub_id}\"\n",
    "\n",
    "eeg_train = np.load(f\"{eeg_dir}/preprocessed_eeg_training.npy\", allow_pickle=True).item()\n",
    "eeg_test = np.load(f\"{eeg_dir}/preprocessed_eeg_test.npy\", allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9ad24771",
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_train_new = np.load(f\"{eeg_dir}/preprocessed_eeg_training_new.npy\", allow_pickle=True)\n",
    "eeg_test_new = np.load(f\"{eeg_dir}/preprocessed_eeg_test_new.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7253eddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16540, 4, 63, 250)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeg_train_new['preprocessed_eeg_data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "383eeb08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16540, 4, 17, 100)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeg_train['preprocessed_eeg_data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c2ec52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ordered_things_image_paths(images_root_dir):\n",
    "    \"\"\"\n",
    "    Generates a list of image paths for the THINGS training set in order.\n",
    "\n",
    "    Args:\n",
    "        images_root_dir (str): Path to the root directory containing the\n",
    "                                'things_images' (or similar) folder.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of full paths to the image files, ordered to match\n",
    "              the preprocessed EEG data's first dimension.\n",
    "    \"\"\"\n",
    "    ordered_image_paths = []\n",
    "    concept_dirs = natsorted(glob.glob(os.path.join(images_root_dir, '*')))\n",
    "\n",
    "    # Iterate through sorted concept directories\n",
    "    for concept_dir in concept_dirs:\n",
    "        if not os.path.isdir(concept_dir):\n",
    "            continue # Skip any non-directory files\n",
    "\n",
    "        # Get and sort image files within each concept\n",
    "        # Assumes images are .jpg or .png, adjust as needed\n",
    "        image_files = natsorted(glob.glob(os.path.join(concept_dir, '*.jpg')))\n",
    "        image_files.extend(natsorted(glob.glob(os.path.join(concept_dir, '*.png'))))\n",
    "        # Remove duplicates if both .jpg and .png are present for the same name (unlikely for THINGS)\n",
    "        image_files = list(set(image_files))\n",
    "        image_files = natsorted(image_files) # Re-sort after set conversion\n",
    "\n",
    "        # Add all images from this concept to the ordered list\n",
    "        ordered_image_paths.extend(image_files)\n",
    "\n",
    "    return ordered_image_paths\n",
    "\n",
    "train_image_dir = os.path.join(image_dir, \"training_images\")\n",
    "test_image_dir = os.path.join(image_dir, \"test_images\")\n",
    "train_image_paths = get_ordered_things_image_paths(train_image_dir)\n",
    "test_image_paths = get_ordered_things_image_paths(test_image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b23d257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16540"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be1db0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "model.eval()\n",
    "def extract_clip_embeddings(image_paths, batch_size=32):\n",
    "    \"\"\"\n",
    "    Extracts CLIP image embeddings from a list of image paths.\n",
    "\n",
    "    Args:\n",
    "        image_paths (list): A list of paths to image files.\n",
    "        batch_size (int): Number of images to process per batch.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A NumPy array of normalized CLIP embeddings.\n",
    "                       Shape: (num_images, embedding_dim)\n",
    "    \"\"\"\n",
    "    all_image_features = []\n",
    "\n",
    "    # Process images in batches to manage memory and speed up\n",
    "    for i in range(0, len(image_paths), batch_size):\n",
    "        batch_paths = image_paths[i : i + batch_size]\n",
    "        images = []\n",
    "        for path in batch_paths:\n",
    "            try:\n",
    "                img = Image.open(path).convert(\"RGB\") # Ensure RGB for consistency\n",
    "                images.append(img)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not open image {path}: {e}. Skipping.\")\n",
    "                continue # Skip problematic images\n",
    "\n",
    "        if not images:\n",
    "            continue # Skip if the batch ended up empty due to errors\n",
    "\n",
    "        # Preprocess images and move inputs to the selected device\n",
    "        inputs = processor(images=images, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "        # Forward pass (image encoder only)\n",
    "        with torch.no_grad():\n",
    "            image_features_batch = model.get_image_features(**inputs)\n",
    "\n",
    "        # Normalize embeddings (critical for alignment)\n",
    "        # Ensure normalization happens on the correct device if needed for fused operations\n",
    "        image_features_batch = image_features_batch / image_features_batch.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Move to CPU and append as NumPy array\n",
    "        all_image_features.append(image_features_batch.cpu().numpy())\n",
    "\n",
    "    # Concatenate all batch results\n",
    "    if all_image_features:\n",
    "        return np.vstack(all_image_features)\n",
    "    else:\n",
    "        return np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ae8eddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_np = extract_clip_embeddings(train_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "463857ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16540, 512)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_image_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cfe7726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 512)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image_np = extract_clip_embeddings(test_image_paths)\n",
    "test_image_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c4a4f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f\"{image_dir}/train_image_clip_embeddings.npy\", train_image_np)\n",
    "np.save(f\"{image_dir}/test_image_clip_embeddings.npy\", test_image_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c499d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGImageDataset(Dataset):\n",
    "    def __init__(self, eeg_data_array, clip_embeddings_array):\n",
    "        if eeg_data_array.shape[0] != clip_embeddings_array.shape[0]:\n",
    "            raise ValueError(\n",
    "                f\"Mismatch in number of conditions/images: \"\n",
    "                f\"EEG has {eeg_data_array.shape[0]}, CLIP has {clip_embeddings_array.shape[0]}\"\n",
    "            )\n",
    "\n",
    "        self.eeg_data = torch.from_numpy(eeg_data_array).float()\n",
    "        self.clip_embeddings = torch.from_numpy(clip_embeddings_array).float()\n",
    "\n",
    "        self.num_total_eeg_trials = self.eeg_data.shape[0] * self.eeg_data.shape[1]\n",
    "        self.eeg_data = self.eeg_data.view(-1, self.eeg_data.shape[2], self.eeg_data.shape[3])\n",
    "\n",
    "        self.clip_embeddings = self.clip_embeddings.repeat_interleave(eeg_data_array.shape[1], dim=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_total_eeg_trials\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'eeg': self.eeg_data[idx],\n",
    "            'clip_embedding': self.clip_embeddings[idx]\n",
    "        }\n",
    "    \n",
    "train_dataset = EEGImageDataset(eeg_train['preprocessed_eeg_data'], train_image_np)\n",
    "test_dataset = EEGImageDataset(eeg_test['preprocessed_eeg_data'], test_image_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "64dbe87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eeg_np = eeg_train['preprocessed_eeg_data'].copy()\n",
    "test_eeg_np = eeg_test['preprocessed_eeg_data'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eacb98ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16540, 4, 17, 100)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_eeg_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5798e464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16540, 512)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_image_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5aa1da58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 100])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]['eeg'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33e78719",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_workers = 4\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    # num_workers=4, \n",
    "    # pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    # num_workers=4, \n",
    "    # pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eb9b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicEEGEncoder(nn.Module):\n",
    "    def __init__(self, in_channels=17, time_points=100, embedding_dim=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=(1, 5), padding=(0, 2)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=(in_channels, 1), padding=0),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "\n",
    "        self.mlp_projector = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.Linear(256, embedding_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        x = self.conv_block(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        eeg_embedding = self.mlp_projector(x)\n",
    "        return eeg_embedding\n",
    "    \n",
    "class ImprovedEEGEncoder(nn.Module):\n",
    "    def __init__(self, in_channels=17, time_points=100, embedding_dim=512):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Temporal Convolution Block ---\n",
    "        # Captures temporal features for each channel\n",
    "        self.temporal_conv_block = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=(1, 5), padding=(0, 2)), # Example: 32 filters, temporal kernel\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=(1, 5), padding=(0, 2)), # More temporal depth\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # --- Spatial Convolution Block ---\n",
    "        # Captures spatial features across channels\n",
    "        # Input to this block will be (Batch, 64, Channels, TimePoints)\n",
    "        self.spatial_conv_block = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=(in_channels, 1)), # Convolve across all channels\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=(1, time_points - 4)), # Flatten remaining temporal dimension\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # Note: kernel_size=(1, time_points - 4) is for (1, X) where X = (100 - (5-1)*2 - (5-1)*2) = 100 - 8 = 92\n",
    "        # (100 is time_points, 5 is kernel_size, 2 is padding for first two convs)\n",
    "        # Better to use AdaptiveAvgPool or calculate dynamically\n",
    "\n",
    "        # Correct way to flatten spatial/temporal output to pass to MLP\n",
    "        # After temporal_conv_block: (B, 64, 17, 100)\n",
    "        # After spatial_conv_block (if it truly reduces to 1x1 or 1xDim): (B, 256, 1, 1) or (B, 256, 1, SomeDim)\n",
    "        # Let's simplify and use AdaptiveAvgPool after each stage for clearer dimensionality.\n",
    "\n",
    "        self.temporal_spatial_features = nn.Sequential(\n",
    "            # Input: (B, 1, C, T)\n",
    "            nn.Conv2d(1, 32, kernel_size=(1, 5), padding=(0, 2)), # Temporal filter 1\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=(1, 5), padding=(0, 2)), # Temporal filter 2\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=(in_channels, 1)), # Spatial filter\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            # Consider a mix of pooling and dense layers here instead of just AdaptiveAvgPool2d\n",
    "            # e.g., nn.AdaptiveAvgPool2d((1, 1)) or nn.MaxPool2d((1, 2))\n",
    "            nn.AdaptiveAvgPool2d((1,1)) # Reduces to (B, 128, 1, 1) -> (B, 128)\n",
    "        )\n",
    "\n",
    "        # Updated MLP Projector\n",
    "        self.mlp_projector = nn.Sequential(\n",
    "            nn.Linear(128, 512), # Increased hidden dim\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Linear(512, embedding_dim),\n",
    "            nn.LayerNorm(embedding_dim) # Add LayerNorm at the final output too\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1) # Add channel dim for Conv2d: (B, 1, C, T)\n",
    "        x = self.temporal_spatial_features(x)\n",
    "        x = x.view(x.size(0), -1) # Flatten to (B, 128) for MLP\n",
    "        eeg_embedding = self.mlp_projector(x)\n",
    "        return eeg_embedding\n",
    "    \n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, lambda_param=0.5, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.lambda_param = lambda_param\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def forward(self, eeg_embeddings, clip_embeddings):\n",
    "        eeg_embeddings = F.normalize(eeg_embeddings, p=2, dim=-1)\n",
    "        clip_embeddings = F.normalize(clip_embeddings, p=2, dim=-1)\n",
    "\n",
    "        logits_per_eeg = torch.matmul(eeg_embeddings, clip_embeddings.T) / self.temperature\n",
    "        logits_per_clip = logits_per_eeg.T\n",
    "\n",
    "        labels = torch.arange(eeg_embeddings.shape[0], device=eeg_embeddings.device)\n",
    "\n",
    "        loss_eeg =  F.cross_entropy(logits_per_eeg, labels)\n",
    "        loss_clip = F.cross_entropy(logits_per_clip, labels)\n",
    "        l_clip = (loss_eeg + loss_clip) / 2\n",
    "\n",
    "        l_mse = F.mse_loss(eeg_embeddings, clip_embeddings)\n",
    "\n",
    "        total_loss = self.lambda_param * l_clip + (1 - self.lambda_param) * l_mse\n",
    "        return total_loss, l_clip.item(), l_mse.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b3c529",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Training on device: {device}\")\n",
    "\n",
    "# eeg_encoder = BasicEEGEncoder(\n",
    "#     in_channels=17,\n",
    "#     time_points=100,\n",
    "#     embedding_dim=512\n",
    "# ).to(device)\n",
    "\n",
    "eeg_encoder = ImprovedEEGEncoder(\n",
    "    in_channels=17,\n",
    "    time_points=100,\n",
    "    embedding_dim=512\n",
    ").to(device)\n",
    "criterion = CombinedLoss(lambda_param=0.1, temperature=0.07).to(device)\n",
    "optimizer = optim.AdamW(eeg_encoder.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cd833e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "  Epoch 1, Batch 100/1034 | Loss: 0.3914 (CLIP: 3.8833, MSE: 0.0034)\n",
      "  Epoch 1, Batch 200/1034 | Loss: 0.3817 (CLIP: 3.7865, MSE: 0.0033)\n",
      "  Epoch 1, Batch 300/1034 | Loss: 0.3757 (CLIP: 3.7274, MSE: 0.0033)\n",
      "  Epoch 1, Batch 400/1034 | Loss: 0.3811 (CLIP: 3.7810, MSE: 0.0033)\n",
      "  Epoch 1, Batch 500/1034 | Loss: 0.3914 (CLIP: 3.8838, MSE: 0.0034)\n",
      "  Epoch 1, Batch 600/1034 | Loss: 0.3904 (CLIP: 3.8735, MSE: 0.0034)\n",
      "  Epoch 1, Batch 700/1034 | Loss: 0.3773 (CLIP: 3.7428, MSE: 0.0033)\n",
      "  Epoch 1, Batch 800/1034 | Loss: 0.3749 (CLIP: 3.7187, MSE: 0.0033)\n",
      "  Epoch 1, Batch 900/1034 | Loss: 0.3906 (CLIP: 3.8755, MSE: 0.0034)\n",
      "  Epoch 1, Batch 1000/1034 | Loss: 0.3749 (CLIP: 3.7192, MSE: 0.0033)\n",
      "\n",
      "Epoch 1 Training Loss: 0.3814 (CLIP: 3.7844, MSE: 0.0033)\n",
      "Epoch 1 Validation Loss: 0.4260 (CLIP: 4.2286, MSE: 0.0035)\n",
      "  Saved best model with validation loss: 0.4260\n",
      "  Epoch 2, Batch 100/1034 | Loss: 0.3707 (CLIP: 3.6774, MSE: 0.0033)\n",
      "  Epoch 2, Batch 200/1034 | Loss: 0.3770 (CLIP: 3.7399, MSE: 0.0034)\n",
      "  Epoch 2, Batch 300/1034 | Loss: 0.3791 (CLIP: 3.7607, MSE: 0.0033)\n",
      "  Epoch 2, Batch 400/1034 | Loss: 0.3646 (CLIP: 3.6165, MSE: 0.0033)\n",
      "  Epoch 2, Batch 500/1034 | Loss: 0.3793 (CLIP: 3.7629, MSE: 0.0034)\n",
      "  Epoch 2, Batch 600/1034 | Loss: 0.3607 (CLIP: 3.5771, MSE: 0.0033)\n",
      "  Epoch 2, Batch 700/1034 | Loss: 0.3721 (CLIP: 3.6907, MSE: 0.0033)\n",
      "  Epoch 2, Batch 800/1034 | Loss: 0.3722 (CLIP: 3.6916, MSE: 0.0034)\n",
      "  Epoch 2, Batch 900/1034 | Loss: 0.3830 (CLIP: 3.8000, MSE: 0.0034)\n",
      "  Epoch 2, Batch 1000/1034 | Loss: 0.3843 (CLIP: 3.8125, MSE: 0.0034)\n",
      "\n",
      "Epoch 2 Training Loss: 0.3783 (CLIP: 3.7523, MSE: 0.0034)\n",
      "Epoch 2 Validation Loss: 0.4262 (CLIP: 4.2312, MSE: 0.0035)\n",
      "  Epoch 3, Batch 100/1034 | Loss: 0.3628 (CLIP: 3.5980, MSE: 0.0033)\n",
      "  Epoch 3, Batch 200/1034 | Loss: 0.3789 (CLIP: 3.7581, MSE: 0.0034)\n",
      "  Epoch 3, Batch 300/1034 | Loss: 0.3748 (CLIP: 3.7178, MSE: 0.0034)\n",
      "  Epoch 3, Batch 400/1034 | Loss: 0.3767 (CLIP: 3.7361, MSE: 0.0034)\n",
      "  Epoch 3, Batch 500/1034 | Loss: 0.3765 (CLIP: 3.7339, MSE: 0.0034)\n",
      "  Epoch 3, Batch 600/1034 | Loss: 0.3714 (CLIP: 3.6840, MSE: 0.0033)\n",
      "  Epoch 3, Batch 700/1034 | Loss: 0.3671 (CLIP: 3.6410, MSE: 0.0034)\n",
      "  Epoch 3, Batch 800/1034 | Loss: 0.3925 (CLIP: 3.8938, MSE: 0.0034)\n",
      "  Epoch 3, Batch 900/1034 | Loss: 0.3801 (CLIP: 3.7708, MSE: 0.0034)\n",
      "  Epoch 3, Batch 1000/1034 | Loss: 0.3861 (CLIP: 3.8302, MSE: 0.0034)\n",
      "\n",
      "Epoch 3 Training Loss: 0.3750 (CLIP: 3.7192, MSE: 0.0034)\n",
      "Epoch 3 Validation Loss: 0.4269 (CLIP: 4.2369, MSE: 0.0035)\n",
      "  Epoch 4, Batch 100/1034 | Loss: 0.3650 (CLIP: 3.6193, MSE: 0.0034)\n",
      "  Epoch 4, Batch 200/1034 | Loss: 0.3621 (CLIP: 3.5903, MSE: 0.0034)\n",
      "  Epoch 4, Batch 300/1034 | Loss: 0.3878 (CLIP: 3.8472, MSE: 0.0034)\n",
      "  Epoch 4, Batch 400/1034 | Loss: 0.3611 (CLIP: 3.5813, MSE: 0.0034)\n",
      "  Epoch 4, Batch 500/1034 | Loss: 0.3731 (CLIP: 3.7002, MSE: 0.0034)\n",
      "  Epoch 4, Batch 600/1034 | Loss: 0.3714 (CLIP: 3.6841, MSE: 0.0034)\n",
      "  Epoch 4, Batch 700/1034 | Loss: 0.3717 (CLIP: 3.6862, MSE: 0.0034)\n",
      "  Epoch 4, Batch 800/1034 | Loss: 0.3995 (CLIP: 3.9637, MSE: 0.0035)\n",
      "  Epoch 4, Batch 900/1034 | Loss: 0.3730 (CLIP: 3.6995, MSE: 0.0034)\n",
      "  Epoch 4, Batch 1000/1034 | Loss: 0.3709 (CLIP: 3.6787, MSE: 0.0034)\n",
      "\n",
      "Epoch 4 Training Loss: 0.3718 (CLIP: 3.6872, MSE: 0.0034)\n",
      "Epoch 4 Validation Loss: 0.4271 (CLIP: 4.2394, MSE: 0.0036)\n",
      "  Epoch 5, Batch 100/1034 | Loss: 0.3623 (CLIP: 3.5930, MSE: 0.0034)\n",
      "  Epoch 5, Batch 200/1034 | Loss: 0.3729 (CLIP: 3.6987, MSE: 0.0034)\n",
      "  Epoch 5, Batch 300/1034 | Loss: 0.3641 (CLIP: 3.6102, MSE: 0.0034)\n",
      "  Epoch 5, Batch 400/1034 | Loss: 0.3509 (CLIP: 3.4785, MSE: 0.0034)\n",
      "  Epoch 5, Batch 500/1034 | Loss: 0.3703 (CLIP: 3.6724, MSE: 0.0034)\n",
      "  Epoch 5, Batch 600/1034 | Loss: 0.3682 (CLIP: 3.6513, MSE: 0.0034)\n",
      "  Epoch 5, Batch 700/1034 | Loss: 0.3780 (CLIP: 3.7488, MSE: 0.0034)\n",
      "  Epoch 5, Batch 800/1034 | Loss: 0.3686 (CLIP: 3.6553, MSE: 0.0034)\n",
      "  Epoch 5, Batch 900/1034 | Loss: 0.3789 (CLIP: 3.7577, MSE: 0.0034)\n",
      "  Epoch 5, Batch 1000/1034 | Loss: 0.3755 (CLIP: 3.7243, MSE: 0.0034)\n",
      "\n",
      "Epoch 5 Training Loss: 0.3686 (CLIP: 3.6556, MSE: 0.0034)\n",
      "Epoch 5 Validation Loss: 0.4289 (CLIP: 4.2569, MSE: 0.0035)\n",
      "  Epoch 6, Batch 100/1034 | Loss: 0.3708 (CLIP: 3.6772, MSE: 0.0034)\n",
      "  Epoch 6, Batch 200/1034 | Loss: 0.3543 (CLIP: 3.5120, MSE: 0.0034)\n",
      "  Epoch 6, Batch 300/1034 | Loss: 0.3648 (CLIP: 3.6179, MSE: 0.0034)\n",
      "  Epoch 6, Batch 400/1034 | Loss: 0.3606 (CLIP: 3.5751, MSE: 0.0034)\n",
      "  Epoch 6, Batch 500/1034 | Loss: 0.3599 (CLIP: 3.5684, MSE: 0.0034)\n",
      "  Epoch 6, Batch 600/1034 | Loss: 0.3392 (CLIP: 3.3623, MSE: 0.0033)\n",
      "  Epoch 6, Batch 700/1034 | Loss: 0.3588 (CLIP: 3.5581, MSE: 0.0034)\n",
      "  Epoch 6, Batch 800/1034 | Loss: 0.3780 (CLIP: 3.7489, MSE: 0.0034)\n",
      "  Epoch 6, Batch 900/1034 | Loss: 0.3654 (CLIP: 3.6234, MSE: 0.0034)\n",
      "  Epoch 6, Batch 1000/1034 | Loss: 0.3626 (CLIP: 3.5959, MSE: 0.0034)\n",
      "\n",
      "Epoch 6 Training Loss: 0.3656 (CLIP: 3.6250, MSE: 0.0034)\n",
      "Epoch 6 Validation Loss: 0.4293 (CLIP: 4.2613, MSE: 0.0035)\n",
      "  Epoch 7, Batch 100/1034 | Loss: 0.3524 (CLIP: 3.4935, MSE: 0.0034)\n",
      "  Epoch 7, Batch 200/1034 | Loss: 0.3640 (CLIP: 3.6090, MSE: 0.0034)\n",
      "  Epoch 7, Batch 300/1034 | Loss: 0.3577 (CLIP: 3.5462, MSE: 0.0034)\n",
      "  Epoch 7, Batch 400/1034 | Loss: 0.3664 (CLIP: 3.6329, MSE: 0.0034)\n",
      "  Epoch 7, Batch 500/1034 | Loss: 0.3679 (CLIP: 3.6482, MSE: 0.0034)\n",
      "  Epoch 7, Batch 600/1034 | Loss: 0.3707 (CLIP: 3.6767, MSE: 0.0034)\n",
      "  Epoch 7, Batch 700/1034 | Loss: 0.3575 (CLIP: 3.5445, MSE: 0.0034)\n",
      "  Epoch 7, Batch 800/1034 | Loss: 0.3599 (CLIP: 3.5683, MSE: 0.0034)\n",
      "  Epoch 7, Batch 900/1034 | Loss: 0.3694 (CLIP: 3.6635, MSE: 0.0034)\n",
      "  Epoch 7, Batch 1000/1034 | Loss: 0.3537 (CLIP: 3.5063, MSE: 0.0034)\n",
      "\n",
      "Epoch 7 Training Loss: 0.3623 (CLIP: 3.5928, MSE: 0.0034)\n",
      "Epoch 7 Validation Loss: 0.4296 (CLIP: 4.2637, MSE: 0.0036)\n",
      "  Epoch 8, Batch 100/1034 | Loss: 0.3543 (CLIP: 3.5128, MSE: 0.0034)\n",
      "  Epoch 8, Batch 200/1034 | Loss: 0.3522 (CLIP: 3.4918, MSE: 0.0034)\n",
      "  Epoch 8, Batch 300/1034 | Loss: 0.3410 (CLIP: 3.3802, MSE: 0.0033)\n",
      "  Epoch 8, Batch 400/1034 | Loss: 0.3599 (CLIP: 3.5687, MSE: 0.0034)\n",
      "  Epoch 8, Batch 500/1034 | Loss: 0.3708 (CLIP: 3.6768, MSE: 0.0034)\n",
      "  Epoch 8, Batch 600/1034 | Loss: 0.3648 (CLIP: 3.6171, MSE: 0.0034)\n",
      "  Epoch 8, Batch 700/1034 | Loss: 0.3708 (CLIP: 3.6770, MSE: 0.0034)\n",
      "  Epoch 8, Batch 800/1034 | Loss: 0.3693 (CLIP: 3.6622, MSE: 0.0034)\n",
      "  Epoch 8, Batch 900/1034 | Loss: 0.3661 (CLIP: 3.6300, MSE: 0.0034)\n",
      "  Epoch 8, Batch 1000/1034 | Loss: 0.3716 (CLIP: 3.6847, MSE: 0.0034)\n",
      "\n",
      "Epoch 8 Training Loss: 0.3594 (CLIP: 3.5635, MSE: 0.0034)\n",
      "Epoch 8 Validation Loss: 0.4308 (CLIP: 4.2759, MSE: 0.0036)\n",
      "  Epoch 9, Batch 100/1034 | Loss: 0.3530 (CLIP: 3.4997, MSE: 0.0034)\n",
      "  Epoch 9, Batch 200/1034 | Loss: 0.3478 (CLIP: 3.4477, MSE: 0.0034)\n",
      "  Epoch 9, Batch 300/1034 | Loss: 0.3598 (CLIP: 3.5676, MSE: 0.0034)\n",
      "  Epoch 9, Batch 400/1034 | Loss: 0.3636 (CLIP: 3.6055, MSE: 0.0034)\n",
      "  Epoch 9, Batch 500/1034 | Loss: 0.3595 (CLIP: 3.5646, MSE: 0.0034)\n",
      "  Epoch 9, Batch 600/1034 | Loss: 0.3577 (CLIP: 3.5470, MSE: 0.0034)\n",
      "  Epoch 9, Batch 700/1034 | Loss: 0.3564 (CLIP: 3.5332, MSE: 0.0034)\n",
      "  Epoch 9, Batch 800/1034 | Loss: 0.3474 (CLIP: 3.4434, MSE: 0.0034)\n",
      "  Epoch 9, Batch 900/1034 | Loss: 0.3628 (CLIP: 3.5975, MSE: 0.0034)\n",
      "  Epoch 9, Batch 1000/1034 | Loss: 0.3564 (CLIP: 3.5330, MSE: 0.0034)\n",
      "\n",
      "Epoch 9 Training Loss: 0.3563 (CLIP: 3.5328, MSE: 0.0034)\n",
      "Epoch 9 Validation Loss: 0.4318 (CLIP: 4.2859, MSE: 0.0036)\n",
      "  Epoch 10, Batch 100/1034 | Loss: 0.3509 (CLIP: 3.4785, MSE: 0.0034)\n",
      "  Epoch 10, Batch 200/1034 | Loss: 0.3272 (CLIP: 3.2417, MSE: 0.0033)\n",
      "  Epoch 10, Batch 300/1034 | Loss: 0.3376 (CLIP: 3.3458, MSE: 0.0034)\n",
      "  Epoch 10, Batch 400/1034 | Loss: 0.3591 (CLIP: 3.5607, MSE: 0.0034)\n",
      "  Epoch 10, Batch 500/1034 | Loss: 0.3655 (CLIP: 3.6247, MSE: 0.0034)\n",
      "  Epoch 10, Batch 600/1034 | Loss: 0.3649 (CLIP: 3.6186, MSE: 0.0034)\n",
      "  Epoch 10, Batch 700/1034 | Loss: 0.3644 (CLIP: 3.6130, MSE: 0.0034)\n",
      "  Epoch 10, Batch 800/1034 | Loss: 0.3670 (CLIP: 3.6392, MSE: 0.0034)\n",
      "  Epoch 10, Batch 900/1034 | Loss: 0.3604 (CLIP: 3.5730, MSE: 0.0034)\n",
      "  Epoch 10, Batch 1000/1034 | Loss: 0.3625 (CLIP: 3.5940, MSE: 0.0034)\n",
      "\n",
      "Epoch 10 Training Loss: 0.3535 (CLIP: 3.5040, MSE: 0.0034)\n",
      "Epoch 10 Validation Loss: 0.4328 (CLIP: 4.2952, MSE: 0.0036)\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    eeg_encoder.train()\n",
    "    total_train_loss = 0\n",
    "    total_l_clip_train = 0\n",
    "    total_l_mse_train = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        eeg = batch['eeg'].to(device)\n",
    "        clip_target = batch['clip_embedding'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        eeg_embeddings = eeg_encoder(eeg)\n",
    "\n",
    "        loss, l_clip_val, l_mse_val = criterion(eeg_embeddings, clip_target)\n",
    "        loss.backward() # Backward pass\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        total_l_clip_train += l_clip_val\n",
    "        total_l_mse_train += l_mse_val\n",
    "\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f\"  Epoch {epoch+1}, Batch {batch_idx+1}/{len(train_loader)} | \"\n",
    "                  f\"Loss: {loss.item():.4f} (CLIP: {l_clip_val:.4f}, MSE: {l_mse_val:.4f})\")\n",
    "            \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    avg_l_clip_train = total_l_clip_train / len(train_loader)\n",
    "    avg_l_mse_train = total_l_mse_train / len(train_loader)\n",
    "    print(f\"\\nEpoch {epoch+1} Training Loss: {avg_train_loss:.4f} (CLIP: {avg_l_clip_train:.4f}, MSE: {avg_l_mse_train:.4f})\")\n",
    "\n",
    "\n",
    "    eeg_encoder.eval() # Set model to evaluation mode\n",
    "    total_val_loss = 0\n",
    "    total_l_clip_val = 0\n",
    "    total_l_mse_val = 0\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculation for validation\n",
    "        for batch_idx, batch in enumerate(test_loader):\n",
    "            eeg = batch['eeg'].to(device)\n",
    "            clip_target = batch['clip_embedding'].to(device)\n",
    "\n",
    "            eeg_embeddings = eeg_encoder(eeg)\n",
    "            loss, l_clip_val, l_mse_val = criterion(eeg_embeddings, clip_target)\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "            total_l_clip_val += l_clip_val\n",
    "            total_l_mse_val += l_mse_val\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(test_loader)\n",
    "    avg_l_clip_val = total_l_clip_val / len(test_loader)\n",
    "    avg_l_mse_val = total_l_mse_val / len(test_loader)\n",
    "    print(f\"Epoch {epoch+1} Validation Loss: {avg_val_loss:.4f} (CLIP: {avg_l_clip_val:.4f}, MSE: {avg_l_mse_val:.4f})\")\n",
    "\n",
    "    # Save best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(eeg_encoder.state_dict(), 'best_eeg_encoder.pth')\n",
    "        print(f\"  Saved best model with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "08c2573e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on device: mps\n",
      "Loaded best EEG encoder for evaluation.\n",
      "Extracted 16000 EEG embeddings and CLIP embeddings from test set.\n"
     ]
    }
   ],
   "source": [
    "clip_embedding_dim = 512 # Or 1024, based on your model's config\n",
    "num_channels = 17\n",
    "num_timepoints = 100\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Evaluation on device: {device}\")\n",
    "\n",
    "eeg_encoder = BasicEEGEncoder(\n",
    "    in_channels=num_channels,\n",
    "    time_points=num_timepoints,\n",
    "    embedding_dim=clip_embedding_dim \n",
    ").to(device)\n",
    "\n",
    "# Load the best saved model weights\n",
    "try:\n",
    "    eeg_encoder.load_state_dict(torch.load('best_eeg_encoder.pth'))\n",
    "    print(\"Loaded best EEG encoder for evaluation.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'best_eeg_encoder.pth' not found. Please train the model first.\")\n",
    "    exit()\n",
    "\n",
    "eeg_encoder.eval() # Set to evaluation mode\n",
    "\n",
    "all_eeg_embeddings = []\n",
    "all_clip_embeddings = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(test_loader): # Use test_loader here\n",
    "        eeg = batch['eeg'].to(device)\n",
    "        clip_target = batch['clip_embedding'].to(device)\n",
    "\n",
    "        eeg_emb = eeg_encoder(eeg)\n",
    "\n",
    "        eeg_emb = F.normalize(eeg_emb, p=2, dim=-1)\n",
    "        clip_target = F.normalize(clip_target, p=2, dim=-1)\n",
    "\n",
    "        all_eeg_embeddings.append(eeg_emb.cpu())\n",
    "        all_clip_embeddings.append(clip_target.cpu())\n",
    "\n",
    "\n",
    "all_eeg_embeddings = torch.cat(all_eeg_embeddings, dim=0).numpy()\n",
    "all_clip_embeddings = torch.cat(all_clip_embeddings, dim=0).numpy()\n",
    "\n",
    "print(f\"Extracted {all_eeg_embeddings.shape[0]} EEG embeddings and CLIP embeddings from test set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2b8dbbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_retrieval_metrics(eeg_embeddings, clip_embeddings, k_values=[1, 5, 10]):\n",
    "    \"\"\"\n",
    "    Calculates Top-K retrieval accuracy and other metrics for EEG embeddings\n",
    "    retrieving their corresponding CLIP embeddings.\n",
    "\n",
    "    Args:\n",
    "        eeg_embeddings (np.ndarray): NumPy array of EEG embeddings.\n",
    "        clip_embeddings (np.ndarray): NumPy array of CLIP embeddings.\n",
    "        k_values (list): A list of integers for which to calculate Top-K accuracy.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A dictionary of accuracies, mean rank, and mean reciprocal rank (MRR).\n",
    "    \"\"\"\n",
    "    num_samples = eeg_embeddings.shape[0]\n",
    "\n",
    "    # Convert NumPy arrays to PyTorch tensors for matrix multiplication\n",
    "    eeg_embeddings_t = torch.from_numpy(eeg_embeddings)\n",
    "    clip_embeddings_t = torch.from_numpy(clip_embeddings)\n",
    "\n",
    "    # Calculate the similarity matrix\n",
    "    # S_ij = similarity(eeg_i, clip_j)\n",
    "    # The result will be a (num_samples x num_samples) matrix where the diagonal\n",
    "    # elements are the similarities between true pairs, and off-diagonals are negatives.\n",
    "    similarity_matrix = torch.matmul(eeg_embeddings_t, clip_embeddings_t.T)\n",
    "\n",
    "    # For each EEG embedding, find the rank of its true corresponding CLIP embedding\n",
    "    # torch.argsort returns the indices that would sort the matrix in descending order.\n",
    "    # So, the first index in each row of sorted_indices is the rank-1 match.\n",
    "    sorted_indices = torch.argsort(similarity_matrix, dim=-1, descending=True)\n",
    "\n",
    "    accuracies = {k: 0 for k in k_values}\n",
    "    ranks = [] # To store the rank of the true positive for each query\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        # The true corresponding CLIP embedding is at the same index 'i' in the clip_embeddings.\n",
    "        # We find its position (rank) within the sorted_indices for the current EEG query.\n",
    "        # `nonzero(as_tuple=True)[0].item()` gets the 0-based index (rank) of 'i' in the sorted list.\n",
    "        rank_of_true_target = (sorted_indices[i] == i).nonzero(as_tuple=True)[0].item()\n",
    "\n",
    "        # Check if the true target is within the Top-K results\n",
    "        for k in k_values:\n",
    "            if rank_of_true_target < k: # 0-based rank, so < k means within top k\n",
    "                accuracies[k] += 1\n",
    "\n",
    "        ranks.append(rank_of_true_target + 1) # Store 1-based rank for mean rank/MRR\n",
    "\n",
    "    # Calculate final percentages\n",
    "    final_accuracies = {}\n",
    "    for k in k_values:\n",
    "        final_accuracies[f\"Top-{k} Accuracy\"] = (accuracies[k] / num_samples) * 100\n",
    "\n",
    "    # Calculate Mean Rank (MR) and Mean Reciprocal Rank (MRR)\n",
    "    mean_rank = np.mean(ranks)\n",
    "    mrr = np.mean([1.0 / rank for rank in ranks])\n",
    "\n",
    "    return final_accuracies, mean_rank, mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "40fc19b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Retrieval Performance Test ---\n",
      "\n",
      "--- Retrieval Performance (EEG to CLIP) ---\n",
      "Top-1 Accuracy: 0.01%\n",
      "Top-5 Accuracy: 0.04%\n",
      "Top-10 Accuracy: 0.11%\n",
      "Top-20 Accuracy: 0.19%\n",
      "Top-50 Accuracy: 0.44%\n",
      "Top-100 Accuracy: 0.81%\n",
      "Mean Rank: 7441.69\n",
      "Mean Reciprocal Rank (MRR): 0.0008\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Running Retrieval Performance Test ---\")\n",
    "\n",
    "# Define the Top-K values you want to evaluate\n",
    "# These should ideally align with what's reported in papers (e.g., 1, 5, 10, 20, 50, 100)\n",
    "evaluation_k_values = [1, 5, 10, 20, 50, 100]\n",
    "\n",
    "retrieval_results, mean_rank_val, mrr_val = calculate_retrieval_metrics(\n",
    "    all_eeg_embeddings,\n",
    "    all_clip_embeddings,\n",
    "    k_values=evaluation_k_values\n",
    ")\n",
    "\n",
    "print(\"\\n--- Retrieval Performance (EEG to CLIP) ---\")\n",
    "for k, acc in retrieval_results.items():\n",
    "    print(f\"{k}: {acc:.2f}%\")\n",
    "print(f\"Mean Rank: {mean_rank_val:.2f}\")\n",
    "print(f\"Mean Reciprocal Rank (MRR): {mrr_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5c410e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
